import pandas as pd                                           # Loading files into pandas dataframe
import numpy as np                                            # To use numpy aggregation functions
from tqdm.auto import tqdm                                    # track loop time
import warnings                                               # Suppress all warnings
warnings.filterwarnings('ignore')
import pickle
import pymysql
import re
import model_shared_utilities as msu                          # helper functions created to create model/plots
import matplotlib.pyplot as plt                               # Generate visualization 
import matplotlib.cm as cm
import altair as alt                                          
import seaborn as sns
from sklearn.preprocessing import StandardScaler              # Transforming dataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.decomposition import NMF, LatentDirichletAllocation    # Model Training
from sklearn.metrics.pairwise import cosine_similarity        # Calculate the similarity between word vectors


def train_model(vec_docs, n_topics, model_type, RANDOM_SEED):
    """
      train an LDA model or NMF model

      Parameters
      ----------
      vec_docs: numerical matrix (tfidf or tf transformation of the text data)
      n_topics (int): number of topics (clusters)
      model_type (str): name of the model to train ('NMF' or 'LDA')
      RANDOM_SEED (int): set model to a random seed for reproducibility

      Returns
      -------
      Trained model
    """ 

    if model_type == 'LDA': 
      model = LatentDirichletAllocation(n_components=n_topics, random_state=RANDOM_SEED, n_jobs=-1)
      model.fit(vec_docs)
    else: 
      model = NMF(n_components=n_topics, random_state=RANDOM_SEED, init="nndsvd")    
      model.fit(vec_docs)

    return model

#----------------------------------------------------------------------------------------------------------------------------
def get_top_terms(model, terms, no_top_words, display=False):
      """
        get the top terms from the trained LDA model or other topics model

        Parameters
        ----------
        model: trained LDA model or other topics model
        terms: list of terms from the vectorizer 
        no_top_words (int): number of top terms to return for each topic
        display: whether or not to display the result

        Returns
        -------
        Top terms from each topic
      """
      top_terms = []
      for topic_idx, topic in enumerate(model.components_):
          term_list = [terms[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
          top_terms.append(term_list)
          if display:
            print("topic %d:" % (topic_idx), term_list)

      return top_terms

#----------------------------------------------------------------------------------------------------------------------------
# this function is taken from the Unsupervised Learning course homework
def topical_coherence(words_list, embeddings_dict):
      """
        Helper function that use in the eval_topic_model() function below.
          Calculate a cosine similarity score of word vectors in the words list 

        Parameters
        ----------
        words_list: words vectors
        embeddings_dict: pretrained words vectors 

        Returns
        -------
        Tuple: 
          mean cosine similarity score, number of words that are not in the pretrained word embedding

      """
      words_not_in_vec = 0
      wordvecs = []
      for w in words_list: 
          try: 
              wordvecs.append(embeddings_dict[w])
          except:
              wordvecs.append(np.array([0]*300))
              words_not_in_vec +=1
              continue 
      wordvecs = np.array(wordvecs)
      
      D = cosine_similarity(wordvecs)
      np.fill_diagonal(D, 0)
      result = D.mean()
      
      return float(result), words_not_in_vec
#----------------------------------------------------------------------------------------------------------------------------
def eval_topic_model(top_terms, embeddings_dict, display=True):
      """
        Evaluate topic model by calculating log-likelihood, preplexity, and coherence score

        Parameters
        ----------
        top_terms: top terms generated by LDA model for each topic 
        embeddings_dict: pretrained words vectors 
        display: whether or not to display the result

        Returns
        -------
        Tuple: 
          coherence score, and number of words not in pretrained embedding word vector 
      """

      # Evaluate the topics by generate a coherence score of the top terms in each topic
      coherence_score = []
      words_not_in_vec = [] 
      for terms in top_terms:
        s, w = topical_coherence(terms, embeddings_dict)
        coherence_score.append(s)
        words_not_in_vec.append(w)
      
      mean_coherence = np.mean(coherence_score)
      total_words_not_in_vec = np.sum(words_not_in_vec)

      if display: 
        print("Overall Topics Coherence score: {:.4f}".format(mean_coherence))
        print("Number of words not in pretrained word vector: {}".format(total_words_not_in_vec))

      return mean_coherence, total_words_not_in_vec
