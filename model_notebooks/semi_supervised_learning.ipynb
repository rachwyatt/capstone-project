{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Laiya Lubben (llubben@umich.edu)\n",
    "<br/>Last updated: Aug 15, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63BLhBE85NdP"
   },
   "source": [
    "# **Initialization - Import / Install libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1627328193327,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "CvUjUnRW5OnG",
    "outputId": "753636e4-9569-4032-899b-aeb144294c54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\laiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\laiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laiya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a list of libraries we need to conduct the analysis:\n",
    "import pandas as pd                                           # Loading files into pandas dataframe\n",
    "import numpy as np                                            # To use numpy aggregation functions\n",
    "from tqdm.auto import tqdm                                    # track loop time\n",
    "import warnings                                               # Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pymysql\n",
    "import re\n",
    "import json\n",
    "import model_shared_utilities as msu                          # helper functions \n",
    "import topic_model_utilities as tm                            # helper functions\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt                               # Generate visualization \n",
    "import matplotlib.cm as cm\n",
    "import altair as alt                                          \n",
    "import seaborn as sns\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler              # Transforming dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier           # Model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "from numpy import concatenate\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "import nltk                                                   # Downloading necessary packagings from nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1627328193328,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "KbHK4uk8sgUp"
   },
   "outputs": [],
   "source": [
    "# This is a magic function to generate the graph within the notebook \n",
    "%matplotlib inline\n",
    "\n",
    "# use this to set random_state to reproduce the same result\n",
    "RANDOM_SEED = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHdKGLW-QHJw"
   },
   "source": [
    "# **Helper Functions**\n",
    "\n",
    "Functions in the ***model_shared_utilities*** module relevant to this notebook: \n",
    "\n",
    "* <font color=\"blue\">get_labels(domain_df, company_df, job_df, dropna=True):</font>\n",
    "  * merge the three dataframes in order to get some labels (domain) for the job dataset based on companies' names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BzgnzWM547h"
   },
   "source": [
    "# **Load / Join Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the secret keys from json file \n",
    "# Note the config file need to be filled with your own credentials before running this notebook \n",
    "# or ask the owners of this repository for the filled config file\n",
    "with open('config.json', 'r') as f:\n",
    "    secret = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4522,
     "status": "ok",
     "timestamp": 1627328197843,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "4BKcfB2_5Zw1"
   },
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "connection = pymysql.connect(host=secret['host'],\n",
    "                              user=secret['user'],\n",
    "                              password=secret['password'],\n",
    "                              database=secret['database'],\n",
    "                              port=secret['port'],\n",
    "                              charset=secret['charset'],\n",
    "                              cursorclass=pymysql.cursors.DictCursor)\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM jd;\")\n",
    "table = cursor.fetchall()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM companies;\")\n",
    "table2 = cursor.fetchall()\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16432,
     "status": "ok",
     "timestamp": 1627328214273,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "HesGQ34D5aXu",
    "outputId": "aebaf918-6cc5-423d-804a-33ed303f10ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job dataset has 18087 rows of data\n",
      "Company dataset has 1024219 rows of data\n"
     ]
    }
   ],
   "source": [
    "# Now let's put the tables into pandas dataframe \n",
    "job_df = pd.DataFrame(table)\n",
    "job_df = job_df.dropna(subset=['job_description'])\n",
    "print(\"Job dataset has\", job_df.shape[0], \"rows of data\")\n",
    "\n",
    "company_df = pd.DataFrame(table2)\n",
    "print(\"Company dataset has\", company_df.shape[0], \"rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 46709,
     "status": "ok",
     "timestamp": 1627328274292,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "NB_4gTls01sL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10399 matches\n"
     ]
    }
   ],
   "source": [
    "# let's load the list of domains we have defined/condensed from the industry column from company_df  \n",
    "filename = 'domain_list.xlsx'\n",
    "domain_df = pd.read_excel(filename)\n",
    "\n",
    "# extracting the domains for the job postings\n",
    "final_df = msu.get_labels(domain_df, company_df, job_df, dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1627342745319,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "Nuc81tiw1Ie9"
   },
   "outputs": [],
   "source": [
    "# creating instance of labelencoder\n",
    "# this is to transform the categorical label(domain) into integers\n",
    "labelencoder = LabelEncoder()\n",
    "final_df['label'] = labelencoder.fit_transform(final_df['job_domain'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYjT7Q4eTwi5"
   },
   "source": [
    "# **Model Training**\n",
    "\n",
    "We are able to find domain labels for some of the data in the job dataset by merging the job dataset with the [company dataset](https://www.kaggle.com/peopledatalabssf/free-7-million-company-dataset) based on companies' names. In this notebook, we want to try semi-supervised learning methods - [label spreading](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html) and [label propagation](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html). We will first train a Random Forest Classifier as a basedline to comparing the performance with the semi-learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1627328214275,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "5_KsfjfQyGYw"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000,              # only top n by freq\n",
    "                                   lowercase=True,                  # turn all to lowercase\n",
    "                                   token_pattern=r\"[A-Za-z]{4,15}\", # words with 4 to 15 characters\n",
    "                                   ngram_range=(1,2),               # include 2-word phrases\n",
    "                                   min_df=25,                       # note: absolute count of doc\n",
    "                                   max_df=0.75,                     # note: % of docs\n",
    "                                   stop_words='english')            # default English stopword\n",
    "\n",
    "# transform the cleaned job descriptions\n",
    "X = tfidf_vectorizer.fit_transform(final_df[\"cleaned_jd\"])  \n",
    "y = final_df.label\n",
    "\n",
    "# split the labeled data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26551,
     "status": "ok",
     "timestamp": 1627328355850,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "MbM8xDeCPQ3-",
    "outputId": "8396c360-a8f9-46a3-cf2e-8511d51f1039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 70.577\n"
     ]
    }
   ],
   "source": [
    "# using a random forest classifier as the baseline\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "pred_y = model.predict(X_test)\n",
    "score = accuracy_score(y_test, pred_y)\n",
    "\n",
    "# summarize score\n",
    "print('Random Forest Classifier Accuracy: %.3f' % (score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20781,
     "status": "ok",
     "timestamp": 1627328376626,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "QbPkg6-2B1AO",
    "outputId": "c75df17a-75b3-4603-f3ea-1b5ba3dfaffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Spreading Accuracy: 64.000\n"
     ]
    }
   ],
   "source": [
    "# evaluate label spreading on the semi-supervised learning dataset\n",
    "# split train into labeled and unlabeled\n",
    "X_train_lab, X_test_unlab, y_train_lab, y_test_unlab = train_test_split(X_train, y_train, random_state=RANDOM_SEED, stratify=y_train)\n",
    "\n",
    "# recombine the training input\n",
    "X_train_mixed = concatenate((X_train_lab.toarray(), X_test_unlab.toarray()))\n",
    "\n",
    "# reassign the y_test_unlab labels as \"-1\" for unlabeled data\n",
    "nolabel = [-1 for _ in range(len(y_test_unlab))]\n",
    "\n",
    "# recombine training dataset labels\n",
    "y_train_mixed = concatenate((y_train_lab, nolabel))\n",
    "\n",
    "# define model\n",
    "model = LabelSpreading()\n",
    "model.fit(X_train_mixed, y_train_mixed)\n",
    "\n",
    "# make predictions on hold out test set\n",
    "pred_y = model.predict(X_test)\n",
    "score = accuracy_score(y_test, pred_y)\n",
    "\n",
    "# summarize score\n",
    "print('Label Spreading Accuracy: %.3f' % (score*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92082,
     "status": "ok",
     "timestamp": 1627328468669,
     "user": {
      "displayName": "Laiya Lubben",
      "photoUrl": "",
      "userId": "11817724685368002439"
     },
     "user_tz": 420
    },
    "id": "dLIUKshY8ogG",
    "outputId": "838e91ee-2f8e-4b22-b253-b3de1ee0608b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Propagation Accuracy: 63.231\n"
     ]
    }
   ],
   "source": [
    "# evaluate label propagation on the semi-supervised learning dataset\n",
    "# define model\n",
    "model = LabelPropagation()\n",
    "model.fit(X_train_mixed, y_train_mixed)\n",
    "\n",
    "# make predictions on hold out test set\n",
    "pred_y = model.predict(X_test)\n",
    "score = accuracy_score(y_test, pred_y)\n",
    "\n",
    "# summarize score\n",
    "print('Label Propagation Accuracy: %.3f' % (score*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is seem as though both the label spreading and label propagation generate low accuracy in comparing to the default\n",
    "# random forest classifer, we will just use supervised learning method to predict the domains"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOLgsgpIoG0hGEtQLiGUikj",
   "collapsed_sections": [
    "63BLhBE85NdP",
    "KFKos4Cm6Kzp",
    "5TLXBhDnVfRm",
    "q2cQbUBIy-jC"
   ],
   "name": "semi_supervised_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
